{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Split the data into two separate columns\n",
    "df[['place_name', 'city']] = df['place_name'].str.split(' - ', expand=True)\n",
    "# Print the updated DataFrame\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "# add the budget and keywords features to the dataset\n",
    "df['budget'] = df['budget'].astype(float)\n",
    "df['keywords'] = df['keywords'].apply(lambda x: ' '.join(x))\n",
    "df['features'] = df['keywords'] + ' ' + df['budget'].astype(str)\n",
    "\n",
    "# create a tf-idf matrix for the features column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['features'])\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "def get_similar_places(place_name, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the name\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the places based on the average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['average_rating'].iloc[x[0]], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Filter the places based on keywords and budget\n",
    "    filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
    "    if keywords:\n",
    "        filtered_places = filtered_places[filtered_places['keywords'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
    "\n",
    "    # Select the relevant columns and return the filtered places sorted by average rating\n",
    "    return filtered_places[['place_name', 'place_id', 'average_rating']].sort_values(by='average_rating', ascending=False)\n",
    "get_similar_places('The Sunken City of Heracleion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the recommendation function\n",
    "def get_recommendations(place_name, city=None, budget=None, cosine_sim=cosine_sim):\n",
    "    # filter the dataset based on the city and budget\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    if city:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "\n",
    "    if budget:\n",
    "        df_filtered = df_filtered[df_filtered['budget'] <= budget]\n",
    "\n",
    "    # add the budget and keywords features to the filtered dataset\n",
    "    df_filtered['budget'] = df_filtered['budget'].astype(float)\n",
    "    df_filtered['keywords'] = df_filtered['keywords'].apply(lambda x: ' '.join(x))\n",
    "    df_filtered['features'] = df_filtered['keywords'] + ' ' + df_filtered['budget'].astype(str)\n",
    "\n",
    "    # create a tf-idf matrix for the features column in the filtered dataset\n",
    "    tfidf_matrix_filtered = tfidf.transform(df_filtered['features'])\n",
    "\n",
    "    # compute the pairwise similarity scores for the features\n",
    "    sim_scores = list(enumerate(cosine_sim[indices[place_name]]))\n",
    "\n",
    "    # sort the places based on the similarity scores and average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: (x[1], df_filtered.loc[x[0], 'average_rating']), reverse=True)\n",
    "    # get the indices of the top 10 most similar places\n",
    "    sim_indices = [i for i, _ in sim_scores[1:11]]\n",
    "\n",
    "    # return the names of the top 10 most similar places\n",
    "    return df_filtered.loc[sim_indices, ['place_name', 'budget', 'average_rating', 'keywords']]\n",
    "\n",
    "# example usage\n",
    "get_recommendations('The Sunken City of Heracleion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the recommendation function\n",
    "def get_recommendations(place_name, city=None, budget=None, cosine_sim=cosine_sim):\n",
    "    # filter the dataset based on the city and budget\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    if city:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "\n",
    "    if budget:\n",
    "        df_filtered = df_filtered[df_filtered['budget'] <= budget]\n",
    "\n",
    "    # add the budget and keywords features to the filtered dataset\n",
    "    df_filtered['budget'] = df_filtered['budget'].astype(float)\n",
    "    df_filtered['keywords'] = df_filtered['keywords'].apply(lambda x: ' '.join(x))\n",
    "    df_filtered['features'] = df_filtered['keywords'] + ' ' + df_filtered['budget'].astype(str)\n",
    "\n",
    "    # create a tf-idf matrix for the features column in the filtered dataset\n",
    "    tfidf_matrix_filtered = tfidf.transform(df_filtered['features'])\n",
    "\n",
    "    # compute the pairwise similarity scores for the features\n",
    "    sim_scores = list(enumerate(cosine_sim[indices[place_name]]))\n",
    "\n",
    "    # sort the places based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the indices of the top 10 most similar places\n",
    "    sim_indices = [i for i, _ in sim_scores[1:11]]\n",
    "\n",
    "    # return the names of the top 10 most similar places\n",
    "    return df_filtered.loc[sim_indices, ['place_name', 'budget', 'average_rating', 'keywords']]\n",
    "\n",
    "# example usage\n",
    "get_recommendations('The Sunken City of Heracleion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m places_df[\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m places_df[\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x))\n\u001b[1;32m     17\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m---> 18\u001b[0m place_keyword_vectors \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(places_df[\u001b[39m'\u001b[39;49m\u001b[39mkeywords\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m     20\u001b[0m \u001b[39m# compute cosine similarity matrix between place keyword vectors\u001b[39;00m\n\u001b[1;32m     21\u001b[0m place_similarity \u001b[39m=\u001b[39m cosine_similarity(place_keyword_vectors)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load data\n",
    "places_df = pd.read_csv(\"/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv\")\n",
    "users_df = pd.read_csv(\"/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv\")\n",
    "\n",
    "# create mapping between place names and integer IDs\n",
    "place2id = {place: i for i, place in enumerate(places_df['place_name'].unique())}\n",
    "\n",
    "# convert place names to integer IDs\n",
    "places_df['place_id'] = places_df['place_name'].apply(lambda x: place2id[x])\n",
    "\n",
    "# convert keywords to bag-of-words vectors\n",
    "places_df['keywords'] = places_df['keywords'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = CountVectorizer()\n",
    "place_keyword_vectors = vectorizer.fit_transform(places_df['keywords'].values)\n",
    "\n",
    "# compute cosine similarity matrix between place keyword vectors\n",
    "place_similarity = cosine_similarity(place_keyword_vectors)\n",
    "\n",
    "# convert user interests to binary vector\n",
    "def get_user_vector(user_interests):\n",
    "    user_vector = [0] * len(place2id)\n",
    "    for interest in user_interests:\n",
    "        if interest in place2id:\n",
    "            user_vector[place2id[interest]] = 1\n",
    "    return user_vector\n",
    "\n",
    "# get recommendations for each user\n",
    "n_recommendations = 5\n",
    "user_place_ratings = []\n",
    "for i, row in users_df.iterrows():\n",
    "    user_id = row['User ID']\n",
    "    user_vector = get_user_vector(row.values[1:])\n",
    "    place_ratings = {}\n",
    "    for j, place_name in enumerate(place2id.keys()):\n",
    "        place_vector = place_similarity[j]\n",
    "        rating = (user_vector @ place_vector) / sum(place_vector)\n",
    "        place_ratings[place_name] = rating\n",
    "    for place_name, rating in sorted(place_ratings.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]:\n",
    "        user_place_ratings.append((user_id, place_name, rating))\n",
    "\n",
    "# print recommendations\n",
    "for user_id, place_name, rating in user_place_ratings:\n",
    "    print(f\"User {user_id} might like {place_name} (rating: {rating})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'place_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'place_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m distances, indices \u001b[39m=\u001b[39m knn\u001b[39m.\u001b[39mkneighbors([user_interests], n_neighbors\u001b[39m=\u001b[39mn_neighbors)\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m indices[\u001b[39m0\u001b[39m]:\n\u001b[0;32m---> 40\u001b[0m     place_id \u001b[39m=\u001b[39m train_users\u001b[39m.\u001b[39;49miloc[j][\u001b[39m'\u001b[39;49m\u001b[39mplace_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     41\u001b[0m     rating \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m distances[\u001b[39m0\u001b[39m][j]\n\u001b[1;32m     42\u001b[0m     user_place_ratings\u001b[39m.\u001b[39mappend((user_id, place_id, rating))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'place_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# load data\n",
    "places_df = pd.read_csv(\"/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv\")\n",
    "users_df = pd.read_csv(\"/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv\")\n",
    "\n",
    "# create mapping between interests and integer IDs\n",
    "interests = set()\n",
    "for col in users_df.columns[1:]:\n",
    "    interests |= set(users_df[col].unique())\n",
    "interests = sorted(list(interests))\n",
    "interest2id = {interest: i for i, interest in enumerate(interests)}\n",
    "\n",
    "# create mapping between place names and integer IDs\n",
    "place2id = {place: i for i, place in enumerate(places_df['place_name'].unique())}\n",
    "\n",
    "# convert interests and place names to integer IDs\n",
    "for col in users_df.columns[1:]:\n",
    "    users_df[col] = users_df[col].apply(lambda x: interest2id[x])\n",
    "places_df['place_id'] = places_df['place_name'].apply(lambda x: place2id[x])\n",
    "\n",
    "# split data into training and testing sets\n",
    "train_size = 0.8\n",
    "train_users = users_df.sample(frac=train_size, random_state=42)\n",
    "test_users = users_df.drop(train_users.index)\n",
    "\n",
    "# train KNN model\n",
    "knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn.fit(train_users.iloc[:, 1:])\n",
    "\n",
    "# get recommendations for each user in test set\n",
    "n_neighbors = 5\n",
    "user_place_ratings = []\n",
    "for i, row in test_users.iterrows():\n",
    "    user_id = i\n",
    "    user_interests = row[1:]\n",
    "    distances, indices = knn.kneighbors([user_interests], n_neighbors=n_neighbors)\n",
    "    for j in indices[0]:\n",
    "        place_id = train_users.iloc[j]['place_id']\n",
    "        rating = 1.0 - distances[0][j]\n",
    "        user_place_ratings.append((user_id, place_id, rating))\n",
    "\n",
    "# map place IDs back to their names\n",
    "id2place = {i: place for place, i in place2id.items()}\n",
    "recommendations = []\n",
    "for user_id, place_id, rating in user_place_ratings:\n",
    "    recommendations.append((user_id, id2place[place_id], rating))\n",
    "\n",
    "# print recommendations\n",
    "for user_id, place_name, rating in recommendations:\n",
    "    print(f\"User {user_id} might like {place_name} (rating: {rating})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libreco'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlibreco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetPure, DataInformation\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlibreco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m \u001b[39mimport\u001b[39;00m LightGCN\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlibreco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libreco'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from libreco.data import DatasetPure, DataInformation\n",
    "from libreco.algorithms import LightGCN\n",
    "from libreco.evaluation import evaluate\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "\n",
    "# create user and item ids\n",
    "unique_users = data['place_id'].unique()\n",
    "unique_items = data['keywords'].apply(pd.Series).stack().unique()\n",
    "user2id = {old: new for new, old in enumerate(unique_users)}\n",
    "item2id = {old: new for new, old in enumerate(unique_items)}\n",
    "data['user_id'] = data['place_id'].map(user2id)\n",
    "data['item_id'] = data['keywords'].apply(lambda x: [item2id[i] for i in x])\n",
    "data = data.explode('item_id').reset_index(drop=True)\n",
    "\n",
    "# split data\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# build dataset\n",
    "train_data, data_info = DatasetPure.build_trainset(train_data)\n",
    "test_data = DatasetPure.build_testset(test_data)\n",
    "\n",
    "# initialize model\n",
    "lightgcn = LightGCN(\n",
    "    task=\"ranking\",\n",
    "    data_info=data_info,\n",
    "    loss_type=\"bpr\",\n",
    "    embed_size=16,\n",
    "    n_epochs=3,\n",
    "    lr=1e-3,\n",
    "    batch_size=2048,\n",
    "    num_neg=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# fit model\n",
    "lightgcn.fit(\n",
    "    train_data,\n",
    "    neg_sampling=True,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# do evaluation on test data\n",
    "evaluate(\n",
    "    model=lightgcn,\n",
    "    data=test_data,\n",
    "    neg_sampling=True,\n",
    "    metrics=[\"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")\n",
    "\n",
    "# recommend places for a user\n",
    "user_id = user2id['El Gezira Sporting Club - Cairo']\n",
    "item_ids = np.arange(len(unique_items))\n",
    "recommendations = lightgcn.recommend_user(user=user_id, n_rec=7, item_ids=item_ids)\n",
    "\n",
    "# map item ids to item names\n",
    "id2item = {new: old for old, new in item2id.items()}\n",
    "recommendations = [id2item[i] for i in recommendations]\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1000, 0)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Compute cosine similarity between user features and place features\u001b[39;00m\n\u001b[1;32m     22\u001b[0m places_features \u001b[39m=\u001b[39m places\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpopularity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbudget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(user_features, places_features)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Recommend top-N places for each user\u001b[39;00m\n\u001b[1;32m     26\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[1;32m   1360\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:163\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    156\u001b[0m         X,\n\u001b[1;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 163\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    164\u001b[0m         Y,\n\u001b[1;32m    165\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    166\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    167\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    168\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    169\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n\u001b[1;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[0;32m--> 940\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    946\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    947\u001b[0m     \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    948\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1000, 0)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Define file paths\n",
    "places_file = '/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv'\n",
    "interests_file = '/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv'\n",
    "\n",
    "# Load data\n",
    "with open(places_file) as f:\n",
    "    places = pd.read_csv(f)\n",
    "with open(interests_file) as f:\n",
    "    interests = pd.read_csv(f)\n",
    "\n",
    "# One-hot encode user interests\n",
    "mlb = MultiLabelBinarizer()\n",
    "user_features = pd.DataFrame(mlb.fit_transform(interests.iloc[:, 1:].values),\n",
    "                             columns=mlb.classes_,\n",
    "                             index=interests.iloc[:, 0].values)\n",
    "\n",
    "# Compute cosine similarity between user features and place features\n",
    "places_features = places.drop(['place_id', 'place_name', 'place_type', 'popularity', 'rating_count', 'average_rating', 'budget', 'keywords'], axis=1)\n",
    "similarity_matrix = cosine_similarity(user_features, places_features)\n",
    "\n",
    "# Recommend top-N places for each user\n",
    "N = 5\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    user_id = interests.iloc[i, 0]\n",
    "    similar_places_indices = similarity_matrix[i].argsort()[::-1][:N]\n",
    "    similar_places = places.iloc[similar_places_indices].nlargest(N, 'popularity')\n",
    "    print(f\"Recommendations for User {user_id}:\")\n",
    "    print(similar_places[['place_name', 'popularity', 'average_rating']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1000, 0)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Compute cosine similarity between user features and place features\u001b[39;00m\n\u001b[1;32m     16\u001b[0m places_features \u001b[39m=\u001b[39m places\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpopularity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbudget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(user_features, places_features)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Recommend top-N places for each user\u001b[39;00m\n\u001b[1;32m     20\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[1;32m   1360\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:163\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    156\u001b[0m         X,\n\u001b[1;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 163\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    164\u001b[0m         Y,\n\u001b[1;32m    165\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    166\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    167\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    168\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    169\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n\u001b[1;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[0;32m--> 940\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[1;32m    944\u001b[0m         )\n\u001b[1;32m    946\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    947\u001b[0m     \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    948\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1000, 0)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Load data\n",
    "places = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "interests = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv')\n",
    "\n",
    "# One-hot encode user interests\n",
    "mlb = MultiLabelBinarizer()\n",
    "user_features = pd.DataFrame(mlb.fit_transform(interests.iloc[:, 1:].values),\n",
    "                             columns=mlb.classes_,\n",
    "                             index=interests.iloc[:, 0].values)\n",
    "\n",
    "# Compute cosine similarity between user features and place features\n",
    "places_features = places.drop(['place_id', 'place_name', 'place_type', 'popularity', 'rating_count', 'average_rating', 'budget', 'keywords'], axis=1)\n",
    "similarity_matrix = cosine_similarity(user_features, places_features)\n",
    "\n",
    "# Recommend top-N places for each user\n",
    "N = 5\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    user_id = interests.iloc[i, 0]\n",
    "    similar_places_indices = similarity_matrix[i].argsort()[::-1][:N]\n",
    "    similar_places = places.iloc[similar_places_indices]\n",
    "    print(f\"Recommendations for User {user_id}:\")\n",
    "    print(similar_places[['place_name', 'popularity', 'average_rating']])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Fashion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m places_features \u001b[39m=\u001b[39m places\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplace_type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpopularity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbudget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m user_features \u001b[39m=\u001b[39m interests\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> 23\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(user_features, places_features)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Recommend top-N places for each user\u001b[39;00m\n\u001b[1;32m     26\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[1;32m   1360\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:155\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    147\u001b[0m         X,\n\u001b[1;32m    148\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    156\u001b[0m         X,\n\u001b[1;32m    157\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    158\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    159\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    160\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    161\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    164\u001b[0m         Y,\n\u001b[1;32m    165\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Fashion'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load data set\n",
    "places = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "interests = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv')\n",
    "\n",
    "# Convert keywords to binary features\n",
    "keywords = set()\n",
    "for k in places['keywords']:\n",
    "    keywords.update(eval(k))\n",
    "for k in keywords:\n",
    "    places[k] = places['keywords'].apply(lambda x: int(k in eval(x)))\n",
    "\n",
    "# Convert interests to binary features\n",
    "interests = interests.drop('User ID', axis=1)\n",
    "for k in keywords:\n",
    "    interests[k] = interests.apply(lambda x: int(k in x.values), axis=1)\n",
    "\n",
    "# Compute user-item similarity matrix\n",
    "places_features = places.drop(['place_id', 'place_name', 'place_type', 'popularity', 'rating_count', 'average_rating', 'budget', 'keywords'], axis=1)\n",
    "user_features = interests.values\n",
    "similarity_matrix = cosine_similarity(user_features, places_features)\n",
    "\n",
    "# Recommend top-N places for each user\n",
    "N = 5\n",
    "for i in range(len(interests)):\n",
    "    user_id = i + 1\n",
    "    user_similarities = similarity_matrix[i]\n",
    "    user_top_N = places.iloc[user_similarities.argsort()[::-1][:N]]\n",
    "    print(f\"Recommendations for User {user_id}:\")\n",
    "    print(user_top_N[['place_id', 'place_name', 'place_type']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mloc[[item_id \u001b[39mfor\u001b[39;00m item_id, _ \u001b[39min\u001b[39;00m recommended_items], [\u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     50\u001b[0m \u001b[39m# Test the function by getting the top 5 recommended items for user 1\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m recommended_items \u001b[39m=\u001b[39m get_recommendations(\u001b[39m1\u001b[39;49m, N\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(recommended_items)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mget_recommendations\u001b[0;34m(user_id, N)\u001b[0m\n\u001b[1;32m     27\u001b[0m user_ratings \u001b[39m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mif\u001b[39;00m row[\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m trainset\u001b[39m.\u001b[39mur[user_id]:\n\u001b[1;32m     30\u001b[0m         user_ratings\u001b[39m.\u001b[39mappend((row[\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m], model\u001b[39m.\u001b[39mpredict(user_id, row[\u001b[39m'\u001b[39m\u001b[39mplace_id\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mest))\n\u001b[1;32m     32\u001b[0m \u001b[39m# Sort the user's ratings in descending order\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Read the CSV files into Pandas DataFrames\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "users_df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv')\n",
    "\n",
    "# Compute the TF-IDF matrix for the keywords column in the df DataFrame\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['keywords'])\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD) on the TF-IDF matrix\n",
    "svd = TruncatedSVD(n_components=30, random_state=42)\n",
    "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Compute the item-item similarity matrix using the cosine similarity\n",
    "item_item_sim = cosine_similarity(svd_matrix)\n",
    "\n",
    "# Define a function to get the top N recommended items for a user\n",
    "def get_recommendations(user_id, N=10):\n",
    "    # Get the user's interests from the users_df DataFrame\n",
    "    interests = users_df.iloc[user_id-1][1:].values.tolist()\n",
    "\n",
    "    # Get the user's ratings for all items in the df DataFrame\n",
    "    user_ratings = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['place_id'] in trainset.ur[user_id]:\n",
    "            user_ratings.append((row['place_id'], model.predict(user_id, row['place_id']).est))\n",
    "\n",
    "    # Sort the user's ratings in descending order\n",
    "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N items that the user has not rated yet\n",
    "    recommended_items = []\n",
    "    for item_id, _ in user_ratings:\n",
    "        if item_id not in trainset.ur[user_id]:\n",
    "            recommended_items.append((item_id, item_item_sim[df[df['place_id']==item_id].index[0]],))\n",
    "\n",
    "        if len(recommended_items) >= N:\n",
    "            break\n",
    "\n",
    "    # Sort the recommended items by their similarity to the user's interests\n",
    "    recommended_items.sort(key=lambda x: sum([x[1][df[df['place_id']==item_id].index[0]] for item_id in trainset.ur[user_id]]), reverse=True)\n",
    "\n",
    "    # Return the top N recommended items\n",
    "    return df.loc[[item_id for item_id, _ in recommended_items], ['place_name', 'average_rating']]\n",
    "\n",
    "# Test the function by getting the top 5 recommended items for user 1\n",
    "recommended_items = get_recommendations(1, N=5)\n",
    "print(recommended_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components(100) must be <= n_features(30).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Compute the SVD matrix for dimensionality reduction\u001b[39;00m\n\u001b[1;32m     15\u001b[0m svd \u001b[39m=\u001b[39m TruncatedSVD(n_components\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m svd_matrix \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39;49mfit_transform(tfidf_matrix)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Compute the item-item similarity matrix using the cosine similarity\u001b[39;00m\n\u001b[1;32m     19\u001b[0m item_item_sim \u001b[39m=\u001b[39m cosine_similarity(svd_matrix)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/decomposition/_truncated_svd.py:237\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m>\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 237\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_components(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components\u001b[39m}\u001b[39;00m\u001b[39m) must be <=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m n_features(\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m     U, Sigma, VT \u001b[39m=\u001b[39m randomized_svd(\n\u001b[1;32m    242\u001b[0m         X,\n\u001b[1;32m    243\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m         random_state\u001b[39m=\u001b[39mrandom_state,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m VT\n",
      "\u001b[0;31mValueError\u001b[0m: n_components(100) must be <= n_features(30)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Read the CSV files into Pandas DataFrames\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "users_df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv')\n",
    "\n",
    "# Compute the TF-IDF matrix for the keywords column in the df DataFrame\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['keywords'])\n",
    "\n",
    "# Compute the SVD matrix for dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Compute the item-item similarity matrix using the cosine similarity\n",
    "item_item_sim = cosine_similarity(svd_matrix)\n",
    "\n",
    "# Define a function to get the top N recommended items for a user\n",
    "def get_recommendations(user_id, N=10):\n",
    "    # Get the user's interests from the users_df DataFrame\n",
    "    interests = users_df.iloc[user_id-1][1:].values.tolist()\n",
    "\n",
    "    # Get the user's ratings for all items in the df DataFrame\n",
    "    user_ratings = []\n",
    "    for _, row in df.iterrows():\n",
    "        user_ratings.append((row['place_id'], row['average_rating'], item_item_sim[df[df['place_id']==row['place_id']].index[0]],))\n",
    "\n",
    "    # Sort the user's ratings in descending order of average rating\n",
    "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N items that the user has not rated yet\n",
    "    recommended_items = []\n",
    "    for item_id, _, similarity in user_ratings:\n",
    "        if item_id not in users_df.iloc[user_id-1][1:].index:\n",
    "            recommended_items.append((item_id, similarity))\n",
    "\n",
    "        if len(recommended_items) >= N:\n",
    "            break\n",
    "\n",
    "    # Sort the recommended items by their similarity to the user's interests\n",
    "    recommended_items.sort(key=lambda x: sum([x[1][df[df['place_id']==item_id].index[0]] for item_id in users_df.iloc[user_id-1][1:].index]), reverse=True)\n",
    "\n",
    "    # Return the top N recommended items\n",
    "    return df.loc[[item_id for item_id, _ in recommended_items[:N]], ['place_name', 'average_rating']]\n",
    "\n",
    "# Test the function by getting the top 5 recommended items for user 1\n",
    "recommended_items = get_recommendations(1, N=5)\n",
    "print(recommended_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, Reader, SVD\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['place_id', 'place_name', 'average_rating']], reader)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVD model on the training set\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Get the predictions for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = sum([1 for pred in predictions if round(pred.est) == pred.r_ui]) / len(predictions) * 100\n",
    "print(f'Collaborative filtering accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Compute the item-item similarity matrix using TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['keywords'])\n",
    "item_item_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Define a function to get the top N similar items\n",
    "def get_similar_items(item_id, N=10):\n",
    "    item_idx = df[df['place_id'] == item_id].index[0]\n",
    "    sim_scores = list(enumerate(item_item_sim[item_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:N+1]\n",
    "    item_indices = [x[0] for x in sim_scores]\n",
    "    return df.iloc[item_indices][['place_name', 'average_rating']]\n",
    "\n",
    "# Test the function by getting the top 5 similar items to item 358\n",
    "similar_items = get_similar_items(358, N=5)\n",
    "print(similar_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, Reader, SVD\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Read the CSV files into Pandas DataFrames\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "users_df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/Interests.csv')\n",
    "\n",
    "# Convert the DataFrame to a Surprise Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['place_id', 'place_name', 'average_rating']], reader)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVD model on the training set\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "# Define a function to get the top N recommended items for a user\n",
    "def get_recommendations(user_id, N=10):\n",
    "    # Get the user's interests from the users_df DataFrame\n",
    "    interests = users_df.iloc[user_id-1][1:].values.tolist()\n",
    "\n",
    "    # Compute the TF-IDF matrix for the keywords column in the df DataFrame\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(df['keywords'])\n",
    "\n",
    "    # Compute the item-item similarity matrix using the cosine similarity\n",
    "    item_item_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Get the user's ratings for all items in the df DataFrame\n",
    "    user_ratings = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['place_id'] in trainset.ur[user_id]:\n",
    "            user_ratings.append((row['place_id'], model.predict(user_id, row['place_id']).est))\n",
    "\n",
    "    # Sort the user's ratings in descending order\n",
    "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N items that the user has not rated yet\n",
    "    recommended_items = []\n",
    "    for item_id, _ in user_ratings:\n",
    "        if item_id not in trainset.ur[user_id]:\n",
    "            recommended_items.append((item_id, item_item_sim[df[df['place_id']==item_id].index[0]],))\n",
    "\n",
    "        if len(recommended_items) >= N:\n",
    "            break\n",
    "\n",
    "    # Sort the recommended items by their similarity to the user's interests\n",
    "    recommended_items.sort(key=lambda x: sum([x[1][df[df['place_id']==item_id].index[0]] for item_id in trainset.ur[user_id]]), reverse=True)\n",
    "\n",
    "    # Return the top N recommended items\n",
    "    return df.loc[[item_id for item_id, _ in recommended_items], ['place_name', 'average_rating']]\n",
    "\n",
    "# Test the function by getting the top 5 recommended items for user 1\n",
    "recommended_items = get_recommendations(1, N=5)\n",
    "print(recommended_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/0csdzgrs4gs5qz5648kq8qtr0000gn/T/ipykernel_19644/4227332140.py:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'similar_places'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similar_places'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Calculate the accuracy score for a test place\u001b[39;00m\n\u001b[1;32m     70\u001b[0m place \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mThe Sunken City of Heracleion\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 71\u001b[0m precision, recall, f1_score \u001b[39m=\u001b[39m accuracy_score(place)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy score for \u001b[39m\u001b[39m{\u001b[39;00mplace\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrecision: \u001b[39m\u001b[39m{\u001b[39;00mprecision\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(place, method, k)\u001b[0m\n\u001b[1;32m     51\u001b[0m recommended_places \u001b[39m=\u001b[39m get_similar_places(place)\n\u001b[1;32m     53\u001b[0m \u001b[39m# Get the actual similar places for the test place\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m actual_places \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39msimilar_places\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(place)][[\u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[:k]\n\u001b[1;32m     56\u001b[0m \u001b[39m# Calculate the intersection of the recommended places and actual places\u001b[39;00m\n\u001b[1;32m     57\u001b[0m common_places \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(recommended_places, actual_places, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similar_places'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Split the data into two separate columns\n",
    "df[['place_name', 'city']] = df['place_name'].str.split(' - ', expand=True)\n",
    "\n",
    "# Add the budget and keywords features to the dataset\n",
    "df['budget'] = df['budget'].astype(float)\n",
    "df['keywords'] = df['keywords'].apply(lambda x: ' '.join(x))\n",
    "df['features'] = df['keywords'] + ' ' + df['budget'].astype(str)\n",
    "\n",
    "# Create a tf-idf matrix for the features column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['features'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "\n",
    "def get_similar_places(place_name, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the name\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the places based on the average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['average_rating'].iloc[x[0]], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Filter the places based on keywords and budget\n",
    "    filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
    "    if keywords:\n",
    "        filtered_places = filtered_places[filtered_places['keywords'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
    "\n",
    "    # Select the relevant columns and return the filtered places sorted by average rating\n",
    "    return filtered_places[['place_name', 'place_id', 'average_rating']].sort_values(by='average_rating', ascending=False)\n",
    "\n",
    "# Define a function to calculate the accuracy score\n",
    "def accuracy_score(place, method='top_k', k=10):\n",
    "    # Get the recommended places for the test place\n",
    "    recommended_places = get_similar_places(place)\n",
    "\n",
    "    # Get the actual similar places for the test place\n",
    "    actual_places = df[df['similar_places'].str.contains(place)][['place_name', 'average_rating']].sort_values(by='average_rating', ascending=False)[:k]\n",
    "\n",
    "    # Calculate the intersection of the recommended places and actual places\n",
    "    common_places = pd.merge(recommended_places, actual_places, on='place_name')\n",
    "\n",
    "    # Calculate the precision and recall scores\n",
    "    precision = len(common_places) / len(recommended_places)\n",
    "    recall = len(common_places) / len(actual_places)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # Return the precision, recall, and F1 score\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Calculate the accuracy score for a test place\n",
    "place = 'The Sunken City of Heracleion'\n",
    "precision, recall, f1_score = accuracy_score(place)\n",
    "print(f\"Accuracy score for {place}:\")\n",
    "print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Split the data into two separate columns\n",
    "df[['place_name', 'city']] = df['place_name'].str.split(' - ', expand=True)\n",
    "\n",
    "# add the budget and keywords features to the dataset\n",
    "df['budget'] = df['budget'].astype(float)\n",
    "df['keywords'] = df['keywords'].apply(lambda x: ' '.join(x))\n",
    "df['features'] = df['keywords'] + ' ' + df['budget'].astype(str)\n",
    "\n",
    "# create a tf-idf matrix for the features column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['features'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "\n",
    "def get_similar_places(place_name, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the name\n",
    "    if place_name not in indices:\n",
    "        raise ValueError(f'Place name \"{place_name}\" not found in dataset')\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    if not sim_scores:\n",
    "        raise ValueError(f'No similar places found for \"{place_name}\"')\n",
    "\n",
    "    # Sort the places based on the average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['average_rating'].iloc[x[0]], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Filter the places based on keywords and budget\n",
    "    filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
    "    if keywords:\n",
    "        filtered_places = filtered_places[filtered_places['keywords'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
    "\n",
    "    # Select the relevant columns and return the filtered places sorted by average rating\n",
    "    return filtered_places[['place_name', 'place_id', 'average_rating']].sort_values(by='average_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Split the data into two separate columns\n",
    "df[['place_name', 'city']] = df['place_name'].str.split(' - ', expand=True)\n",
    "\n",
    "# Add the budget and keywords features to the dataset\n",
    "df['budget'] = df['budget'].astype(float)\n",
    "df['keywords'] = df['keywords'].apply(lambda x: ' '.join(x))\n",
    "df['features'] = df['keywords'] + ' ' + df['budget'].astype(str)\n",
    "\n",
    "# Create a tf-idf matrix for the features column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['features'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "\n",
    "def get_similar_places(place_name, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the name\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the places based on the average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['average_rating'].iloc[x[0]], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Filter the places based on keywords and budget\n",
    "    filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
    "    if keywords:\n",
    "        filtered_places = filtered_places[filtered_places['keywords'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
    "\n",
    "    # Select the relevant columns and return the filtered places sorted by average rating\n",
    "    return filtered_places[['place_name', 'place_id', 'average_rating']].sort_values(by='average_rating', ascending=False)\n",
    "\n",
    "# Function to check the accuracy of the recommendations\n",
    "def check_accuracy(place_name, expected_result, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the recommended places\n",
    "    recommended_places = get_similar_places(place_name, keywords, budget, cosine_sim, df)\n",
    "\n",
    "    # Get the actual place names from the expected result\n",
    "    actual_places = df[df['place_name'].isin(expected_result)][['place_name', 'average_rating']]\n",
    "\n",
    "    # Calculate the accuracy score\n",
    "    accuracy = accuracy_score(recommended_places['place_name'], actual_places['place_name'])\n",
    "\n",
    "    # Print the accuracy score\n",
    "    print(f\"Accuracy score for {place_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/0csdzgrs4gs5qz5648kq8qtr0000gn/T/ipykernel_19644/3224720774.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'similar_places'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similar_places'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m recommended_places \u001b[39m=\u001b[39m get_similar_places(place)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Get the actual similar places for the test place\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m actual_places \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39msimilar_places\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(place)][[\u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maverage_rating\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[:\u001b[39m10\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[39m# Calculate the intersection of the recommended places and actual places\u001b[39;00m\n\u001b[1;32m     16\u001b[0m intersection \u001b[39m=\u001b[39m recommended_places\u001b[39m.\u001b[39mmerge(actual_places, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplace_name\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similar_places'"
     ]
    }
   ],
   "source": [
    "# Get the list of place names to test\n",
    "test_places = ['The Sunken City of Heracleion', 'Some other place', 'Another place']\n",
    "\n",
    "# Create an empty list to store the accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "# Iterate over the test places\n",
    "for place in test_places:\n",
    "    # Get the recommended places for the test place\n",
    "    recommended_places = get_similar_places(place)\n",
    "    \n",
    "    # Get the actual similar places for the test place\n",
    "    actual_places = df[df['similar_places'].str.contains(place)][['place_name', 'average_rating']].sort_values(by='average_rating', ascending=False)[:10]\n",
    "    \n",
    "    # Calculate the intersection of the recommended places and actual places\n",
    "    intersection = recommended_places.merge(actual_places, on='place_name')\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    accuracy_score = intersection.shape[0] / 10\n",
    "    \n",
    "    # Append the accuracy score to the list\n",
    "    accuracy_scores.append(accuracy_score)\n",
    "\n",
    "# Calculate the mean accuracy score\n",
    "mean_accuracy_score = np.mean(accuracy_scores)\n",
    "\n",
    "print(f'Mean accuracy score: {mean_accuracy_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for The Sunken City of Heracleion: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/0csdzgrs4gs5qz5648kq8qtr0000gn/T/ipykernel_19644/3224720774.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "check_accuracy('The Sunken City of Heracleion', ['The Pyramids of Giza', 'The Valley of the Kings', 'The Great Sphinx'], ['history', 'ancient'], budget=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/GraduationProject/Code/GraduationProject/DataSets/places_data.csv')\n",
    "\n",
    "# Split the data into two separate columns\n",
    "df[['place_name', 'city']] = df['place_name'].str.split(' - ', expand=True)\n",
    "\n",
    "# add the budget and keywords features to the dataset\n",
    "df['budget'] = df['budget'].astype(float)\n",
    "df['keywords'] = df['keywords'].apply(lambda x: ' '.join(x))\n",
    "df['features'] = df['keywords'] + ' ' + df['budget'].astype(str)\n",
    "\n",
    "# create a tf-idf matrix for the features column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['features'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "\n",
    "def get_similar_places(place_name, keywords=[], budget=np.inf, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the name\n",
    "    if place_name not in indices:\n",
    "        raise ValueError(f'Place name \"{place_name}\" not found in dataset')\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    if not sim_scores:\n",
    "        raise ValueError(f'No similar places found for \"{place_name}\"')\n",
    "    print(f'Similar places for \"{place_name}\": {sim_scores}')\n",
    "\n",
    "    # Sort the places based on the average rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['average_rating'].iloc[x[0]], reverse=True)\n",
    "    print(f'Sorted places for \"{place_name}\": {sim_scores}')\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Filter the places based on keywords and budget\n",
    "    filtered_places = df.iloc[place_indices][df['budget'] <= budget]\n",
    "    if keywords:\n",
    "        filtered_places = filtered_places[filtered_places['keywords'].apply(lambda x: any(keyword in x for keyword in keywords))]\n",
    "\n",
    "    # Select the relevant columns and return the filtered places sorted by average rating\n",
    "    return filtered_places[['place_name', 'place_id', 'average_rating']].sort_values(by='average_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with the data you provided\n",
    "data = {'place': }\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the 'place' column into two columns using '-' as the separator\n",
    "df[['place_name', 'city']] = df['place'].str.split('-', n=1, expand=True)\n",
    "\n",
    "# Strip leading and trailing whitespaces from the 'city' column\n",
    "df['city'] = df['city'].str.strip()\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/Graduation Project/DS/places csv.csv')\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "df['place_name'] = df['place_name'].fillna('')\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(df['place_name'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Construct a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name']).drop_duplicates()\n",
    "\n",
    "# Define a function that takes in place name as input and outputs the top 10 most similar places sorted by rating\n",
    "def get_similar_places(place_name, cosine_sim=cosine_sim, df=df):\n",
    "    # Get the index of the place that matches the title\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwise similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the places based on the rating\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: df['rating'].iloc[x[0]], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 10 most similar places\n",
    "    place_indices = [x[0] for x in sim_scores[1:11]]\n",
    "\n",
    "    # Return the top 10 most similar places sorted by rating\n",
    "    return df.iloc[place_indices].sort_values(by='rating', ascending=False)\n",
    "\n",
    "# Test the function with some sample inputs\n",
    "similar_places = get_similar_places('Rooftop Lounge & Bar, Alexandria')\n",
    "print(similar_places)\n",
    "\n",
    "similar_places = get_similar_places('Wunder Garten, Alexandria')\n",
    "print(similar_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df1=pd.read_csv('/Users/rewanabdelqader/Collage/Semster 8/Graduation Project/DS/places csv.csv')\n",
    "df1.head(5)\n",
    "\n",
    "place_id\tplace_name\trating\tpopularity\trating_count\taverage_rating\tbudget\tkeywords\n",
    "0\t1\tMontaza Palace Gardens\t2\t174.313947\t400\t4.0\t300\t['cafe', 'dinner', 'lunch', 'coffee', 'brunch'...\n",
    "1\t2\tExit Games Egypt - Cairo\t2\t170.926290\t200\t5.0\t100\t['cafe', 'dinner', 'pastries', 'breakfast', 't...\n",
    "2\t3\tRoasting House - New Cairo, Cairo\t1\t142.719149\t500\t5.0\t300\t['tea', 'cafe', 'lunch', 'desserts']\n",
    "3\t4\tKharga Oasis\t3\t184.004065\t300\t3.5\t100\t['coffee']\n",
    "4\t5\tThe Secret Chambers Egypt - Cairo\t2\t156.373940\t500\t4.0\t300\t['dinner', 'tea', 'desserts', 'lunch', 'coffee..\n",
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "df1['place_name'] = df1['place_name'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(df1['place_name'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape\n",
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(df1.index, index=df1['place_name']).drop_duplicates()\n",
    "# Function that takes in place title as input and outputs most similar movies\n",
    "def get_recommendations(place_name, cosine_sim=cosine_sim):\n",
    "    # Get the index of the place that matches the title\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all places with that place\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar place\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the place indices\n",
    "    places_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df1['place_name'].iloc[places_indices]\n",
    "    get_recommendations('Rooftop Lounge & Bar, Alexandria')\n",
    "    181       Pegasus Lounge Bar, Alexandria\n",
    "128    Sky Roof Bar & Lounge, Alexandria\n",
    "303                      9 Lounge, Cairo\n",
    "155                       The Bar, Cairo\n",
    "399            The Lodge Bar, Alexandria\n",
    "191       Coffee Lounge - Zamalek, Cairo\n",
    "40                     Buddha-Bar, Cairo\n",
    "126           Mojo Lounge & Grill, Cairo\n",
    "70                   I Bistro Bar, Cairo\n",
    "25                       Roof Bar, Cairo\n",
    "Name: place_name, dtype: object\n",
    "get_recommendations('Wunder Garten, Alexandria')\n",
    "95       Escape It Egypt - Alexandria\n",
    "275         Escape Egypt - Alexandria\n",
    "278       The Room Egypt - Alexandria\n",
    "471                    Alexandria Zoo\n",
    "462        The Key Egypt - Alexandria\n",
    "309          Rooms Egypt - Alexandria\n",
    "280            Alexandria City Center\n",
    "351              Cap d'Or, Alexandria\n",
    "36     Escape Room Egypt - Alexandria\n",
    "185       Breakout Egypt - Alexandria\n",
    "Name: place_name, dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load user data\n",
    "user_data = pd.read_csv('/Users/rewanabdelqader/Collage/Semster8/Graduation_Project/DS/Fake Data/User Data.csv')\n",
    "\n",
    "# Create user profile vector\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "user_profile = tfidf_vectorizer.fit_transform(user_data.values.astype('U'))\n",
    "\n",
    "# Load hangout data\n",
    "hangout_data = pd.read_csv('hangout_data.csv')\n",
    "\n",
    "\n",
    "# Create hangout profile vector\n",
    "hangout_profile = tfidf_vectorizer.fit_transform(hangout_data['description'])\n",
    "\n",
    "# Calculate similarity between user profile and hangout profile\n",
    "cosine_similarities = cosine_similarity(user_profile, hangout_profile)\n",
    "\n",
    "# Get hangout recommendations for each user\n",
    "hangout_recommendations = {}\n",
    "for i, row in user_data.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    similarity_scores = list(enumerate(cosine_similarities[i]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[1:6] # Top 5 similar hangouts\n",
    "    hangout_indices = [i[0] for i in similarity_scores]\n",
    "    hangout_recommendations[user_id] = list(hangout_data.iloc[hangout_indices]['hangout_name'])\n",
    "\n",
    "# Print hangout recommendations for each user\n",
    "for user_id, recommendations in hangout_recommendations.items():\n",
    "    print(f\"Recommendations for user {user_id}:\")\n",
    "    print(', '.join(recommendations))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load user data\n",
    "user_interests = pd.read_csv('user_interests.csv')\n",
    "user_behavior = pd.read_csv('user_behavior.csv')\n",
    "user_photos = pd.read_csv('user_photos.csv')\n",
    "\n",
    "# Combine user data into a single dataframe\n",
    "user_data = pd.concat([user_interests, user_behavior, user_photos], axis=1)\n",
    "\n",
    "# Clean user data\n",
    "user_data = user_data.fillna('')\n",
    "\n",
    "# Create user profile vector\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "user_profile = tfidf_vectorizer.fit_transform(user_data.values.astype('U'))\n",
    "\n",
    "# Load destination data\n",
    "destination_data = pd.read_csv('destination_data.csv')\n",
    "\n",
    "# Clean destination data\n",
    "destination_data = destination_data.fillna('')\n",
    "\n",
    "# Create destination profile vector\n",
    "destination_profile = tfidf_vectorizer.transform(destination_data.values.astype('U'))\n",
    "\n",
    "# Calculate cosine similarity between user profile and destination profiles\n",
    "similarity_scores = cosine_similarity(user_profile, destination_profile)\n",
    "\n",
    "# Create a list of recommended destinations for each user\n",
    "recommendations = []\n",
    "for i in range(len(user_data)):\n",
    "    top_destinations = np.argsort(similarity_scores[i])[::-1][:10]\n",
    "    recommendations.append(list(destination_data.iloc[top_destinations]['destination_name']))\n",
    "\n",
    "# Save recommendations to a file\n",
    "with open('user_recommendations.csv', 'w') as f:\n",
    "    f.write('user_id,recommendations\\n')\n",
    "    for i in range(len(user_data)):\n",
    "        f.write(f'{i+1},\"{\", \".join(recommendations[i])}\"\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new tf-idf matrix for the features column\n",
    "tfidf_matrix_features = tfidf.fit_transform(df['features'])\n",
    "\n",
    "# compute the cosine similarity matrix\n",
    "cosine_sim_features = linear_kernel(tfidf_matrix_features, tfidf_matrix_features)\n",
    "\n",
    "# create a reverse map of indices and place names\n",
    "indices = pd.Series(df.index, index=df['place_name'])\n",
    "\n",
    "# define the recommendation function\n",
    "def get_recommendations(place_name, cosine_sim_name=cosine_sim_name, cosine_sim_features=cosine_sim_features):\n",
    "    # get the index of the place that matches the name\n",
    "    idx = indices[place_name]\n",
    "\n",
    "    # compute the pairwise similarity scores for the name and features\n",
    "    sim_scores_name = list(enumerate(cosine_sim_name[idx]))\n",
    "    sim_scores_features = list(enumerate(cosine_sim_features[idx]))\n",
    "\n",
    "    # combine the two similarity scores by taking their average\n",
    "    sim_scores = [(i, (score_name + score_feat) / 2)\n",
    "                  for (i, score_name), (_, score_feat)\n",
    "                  in zip(sim_scores_name, sim_scores_features)]\n",
    "\n",
    "    # sort the places based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the indices of the top 10 most similar places\n",
    "    sim_indices = [i for i, _ in sim_scores[1:11]]\n",
    "\n",
    "    # return the names of the top 10 most similar places\n",
    "    return df.loc[sim_indices, 'place_name']\n",
    "\n",
    "# example usage\n",
    "get_recommendations('The Sunken City of Heracleion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(place_name, city=None, budget=None, cosine_sim=cosine_sim):\n",
    "    # filter the dataset based on the city and budget\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    if city:\n",
    "        df_filtered = df_filtered[df_filtered['city'] == city]\n",
    "\n",
    "    if budget:\n",
    "        df_filtered = df_filtered[df_filtered['budget'] <= budget]\n",
    "\n",
    "    # add the budget and keywords features to the filtered dataset\n",
    "    df_filtered['budget'] = df_filtered['budget'].astype(float)\n",
    "    df_filtered['keywords'] = df_filtered['keywords'].apply(lambda x: ' '.join(x))\n",
    "    df_filtered['features'] = df_filtered['keywords'] + ' ' + df_filtered['budget'].astype(str)\n",
    "\n",
    "    # create a tf-idf matrix for the features column in the filtered dataset\n",
    "    tfidf_matrix_filtered = tfidf.transform(df_filtered['features'])\n",
    "\n",
    "    # compute the pairwise similarity scores for the features\n",
    "    sim_scores = list(enumerate(cosine_sim[indices[place_name]]))\n",
    "\n",
    "    # sort the places based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the indices of the top 10 most similar places\n",
    "    sim_indices = [i for i, _ in sim_scores[1:11]]\n",
    "\n",
    "    # filter the places based on the number of similar keywords\n",
    "    similar_keywords = set(df.loc[indices[place_name], 'keywords'])\n",
    "    recommended_indices = []\n",
    "    for index in sim_indices:\n",
    "        if len(set(df_filtered.loc[index, 'keywords']).intersection(similar_keywords)) >= 3:\n",
    "            recommended_indices.append(index)\n",
    "\n",
    "    # return the names of the recommended places\n",
    "    return df_filtered.loc[recommended_indices, ['place_name', 'budget', 'average_rating', 'keywords']]\n",
    "get_recommendations('The Sunken City of Heracleion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
